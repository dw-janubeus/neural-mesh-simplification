{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef23eb0-4fc4-4f75-bc46-de36bdceeb1b",
   "metadata": {},
   "source": [
    "# Train the Neural Mesh Simplification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68548e86-1a2b-4ab7-b89e-5c93314f9345",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c589dd-aa96-4d93-be4e-6cfdd181f10f",
   "metadata": {},
   "source": [
    "### [*only required for remote runs*] Remote environment setup\n",
    "\n",
    "If you are running this notebook remotely (e.g. Google Colab), you'll want to set up the environment by\n",
    "* Downloading the repository from GitHub\n",
    "* Setting up the python environment\n",
    "\n",
    "If are opening this notebook locally, by running `jupyter lab` from the repository root and the right conda environment activated, the above step is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179368da-e6fc-4beb-ae82-ca18137de974",
   "metadata": {},
   "source": [
    "#### Step 1. Check out the repo\n",
    "That's where the source code for mesh simplification, along with its dependency definitions and other utilities, lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf37ac-a233-44f9-8f6c-f929566a0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dw-janubeus/neural-mesh-simplification.git neural-mesh-simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db897c-f4e0-416e-b92c-392716730e07",
   "metadata": {},
   "source": [
    "#### Step 2. Install python version 3.12 using apt-get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1795ff",
   "metadata": {},
   "source": [
    "Check the current python version by running the following command. This notebook requires Python 3.12 to run. Either install it via your Notebook environment settings and jump to Step 6 or follow all the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6be6e0-41b0-43ca-9fda-508a8cb198be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install python3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e15c47-fcff-4e5b-8692-ff62fe0cc764",
   "metadata": {},
   "source": [
    "#### Step 3. Update alternatives to use the new Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36faa2-8839-4476-b74d-ffbf4f2f086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1\n",
    "!sudo update-alternatives --config python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5e4ec-3074-4041-bdee-5246987b4ae0",
   "metadata": {},
   "source": [
    "#### Step 4. Install pip and the required packages for the new Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11a192-478c-4797-b9ba-42c41f9e9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f get-pip*.py\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python get-pip.py\n",
    "!python -m pip install ipykernel\n",
    "!python -m ipykernel install --user --name python3.12 --display-name \"Python 3.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3d62d-f255-4f2e-830f-09080b41d364",
   "metadata": {},
   "source": [
    "#### Step 5. Restart and verify\n",
    "At this point you may need to restart the session, after which you want to verify that `python` is at the right version (`3.12`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29e1e1-5e0d-4dbf-b4cd-c5a26e5092b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd709a2-ebec-48fb-b5b7-2e75a9fbb129",
   "metadata": {},
   "source": [
    "#### Step 6. Upgrade pip and setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915962f0-9dff-40d3-a4f3-cbc1cf69fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install --upgrade build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5242f0-c6de-4c1c-9922-eaf4ee161e35",
   "metadata": {},
   "source": [
    "### Set repository as the working directory \n",
    "CD into the repository downloaded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ca79a-83e8-418b-85e4-364ca8c6b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd neural-mesh-simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b48728-47c7-4441-8043-258e71f2a8d1",
   "metadata": {},
   "source": [
    "### Package requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccef76-a2ed-47a5-8c7b-bc7c14ba6770",
   "metadata": {},
   "source": [
    "Depending on whether you are using PyTorch on a CPU or a GPU,\n",
    "you'll have to use the correct binaries for PyTorch and the PyTorch Geometric libraries. You can install them via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141322d-334d-4957-b371-0660a7f7dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install torch_cluster==1.6.3 torch_geometric==2.5.3 torch_scatter==2.1.2 torch_sparse==0.6.18 -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d234c-1c03-445a-b2f7-d56fd42e4f94",
   "metadata": {},
   "source": [
    "Replace “cu121” with the appropriate CUDA version for your system. If you don't know what is your cuda version, run `nvidia-smi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20093e-689c-40ab-8f2e-c86337dbc466",
   "metadata": {},
   "source": [
    "Only then you can install the requirements via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fa547-f157-4679-a6c1-f0304ac59aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install GPUtil  # For GPU monitoring in Colab\n",
    "!pip uninstall -y neural-mesh-simplification\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d21495-9bd4-46df-a4c8-a31b573e3d79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Download the training data\n",
    "We can use the Hugging Face API to download some mesh data to use for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42801141-20d8-40da-8f59-b7b25338ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "target_folder = \"data/raw\"\n",
    "wip_folder = os.path.join(target_folder, \"wip\")\n",
    "os.makedirs(wip_folder, exist_ok=True)\n",
    "\n",
    "# abc_train is really large (+5k meshes), so download just a sample\n",
    "folder_patterns = [\"abc_extra_noisy/03_meshes/*.ply\", \"abc_train/03_meshes/*.ply\"]\n",
    "\n",
    "# Download\n",
    "snapshot_download(\n",
    "    repo_id=\"perler/ppsurf\",\n",
    "    repo_type=\"dataset\",\n",
    "    cache_dir=wip_folder,\n",
    "    allow_patterns=folder_patterns[0],\n",
    ")\n",
    "\n",
    "# Move files from wip folder to target folder\n",
    "for root, _, files in os.walk(wip_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ply\"):\n",
    "            src_file = os.path.join(root, file)\n",
    "            dest_file = os.path.join(target_folder, file)\n",
    "            shutil.copy2(src_file, dest_file)\n",
    "            os.remove(src_file)\n",
    "\n",
    "# Remove the wip folder\n",
    "shutil.rmtree(wip_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df29f3-ee23-47d3-b987-7b3f8e0cfc66",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "The downloaded data needs to be prepapared for training. We can use a script in the repository we checked out for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe14c3-7ca1-4a47-80ab-a5d049def2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/processed\n",
    "!python scripts/preprocess_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe1228-5a40-4f44-9e58-5607beabd5a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3af882-fe98-48d4-bd34-cf16912cc5d4",
   "metadata": {},
   "source": [
    "When using a GPU, ensure the training is happening on the GPU, and the environment is configured properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cf962-81a4-41b4-a62a-9431d0ed6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# GPU verification and memory management for Colab T4\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Clear cache and optimize for T4\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "print(f\"\\nSystem RAM: {psutil.virtual_memory().total / 1024**3:.2f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n",
    "\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad8fd1-10a8-410c-b826-848c91a227bc",
   "metadata": {},
   "source": [
    "### Setup Google Drive (Optional)\n",
    "Mount Google Drive to persist checkpoints across Colab sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to mount Google Drive for checkpoint persistence\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Set checkpoint directory to Google Drive (uncomment if using Drive)\n",
    "# checkpoint_dir = '/content/drive/MyDrive/neural-mesh-checkpoints'\n",
    "# !mkdir -p \"{checkpoint_dir}\"\n",
    "\n",
    "# Or use local storage (will be lost when session ends)\n",
    "checkpoint_dir = 'checkpoints'\n",
    "!mkdir -p {checkpoint_dir}\n",
    "\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-config",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "Configure training parameters optimized for Colab T4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for T4\n",
    "import os\n",
    "\n",
    "# Check available data\n",
    "data_path = \"data/processed\"\n",
    "if os.path.exists(data_path):\n",
    "    data_files = os.listdir(data_path)\n",
    "    print(f\"Found {len(data_files)} processed data files\")\n",
    "else:\n",
    "    print(\"Warning: Processed data directory not found!\")\n",
    "\n",
    "# Set training arguments\n",
    "config_path = \"configs/default.yaml\"\n",
    "resume_checkpoint = None  # Set to path if resuming training\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Config path: {config_path}\")\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "print(f\"Resume from: {resume_checkpoint or 'None (fresh training)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad8fd1-10a8-410c-b826-848c91a227bc",
   "metadata": {},
   "source": [
    "### Start Training with Enhanced Arguments\n",
    "Using all available training script features optimized for Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef712306-3971-421b-a3ee-d063759c926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training command with all available arguments\n",
    "cmd = f\"python scripts/train.py \\\n",
    "    --data-path {data_path} \\\n",
    "    --config {config_path} \\\n",
    "    --checkpoint-dir {checkpoint_dir} \\\n",
    "    --monitor \\\n",
    "    --debug\"\n",
    "\n",
    "# Add resume option if checkpoint exists\n",
    "if resume_checkpoint:\n",
    "    cmd += f\" --resume {resume_checkpoint}\"\n",
    "\n",
    "print(f\"Training command:\\n{cmd}\\n\")\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "post-training",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-analysis",
   "metadata": {},
   "source": [
    "### Check Available Checkpoints\n",
    "List and analyze saved checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-checkpoints",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Find all checkpoint files\n",
    "checkpoint_pattern = os.path.join(checkpoint_dir, '*.pth')\n",
    "checkpoints = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "    for cp in sorted(checkpoints):\n",
    "        size_mb = os.path.getsize(cp) / (1024*1024)\n",
    "        print(f\"  - {os.path.basename(cp)} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Use the most recent checkpoint\n",
    "    latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "    print(f\"\\nMost recent checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Training may have failed or not started.\")\n",
    "    latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Evaluate the trained model on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_checkpoint:\n",
    "    # Run evaluation on test set\n",
    "    eval_cmd = f\"python scripts/evaluate.py \\\n",
    "        --eval-data-path {data_path} \\\n",
    "        --checkpoint {latest_checkpoint} \\\n",
    "        --config {config_path}\"\n",
    "    \n",
    "    print(f\"Evaluation command:\\n{eval_cmd}\\n\")\n",
    "    print(\"Running evaluation...\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    !{eval_cmd}\n",
    "else:\n",
    "    print(\"No checkpoint available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-section",
   "metadata": {},
   "source": [
    "### Inference Example\n",
    "Test the trained model on a sample mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_checkpoint:\n",
    "    # Find a sample mesh file\n",
    "    sample_meshes = glob.glob(\"data/raw/*.ply\")\n",
    "    \n",
    "    if sample_meshes:\n",
    "        sample_mesh = sample_meshes[0]\n",
    "        output_mesh = \"simplified_sample.obj\"\n",
    "        \n",
    "        infer_cmd = f\"python scripts/infer.py \\\n",
    "            --input-file {sample_mesh} \\\n",
    "            --output-file {output_mesh} \\\n",
    "            --model-checkpoint {latest_checkpoint} \\\n",
    "            --device cuda\"\n",
    "        \n",
    "        print(f\"Inference command:\\n{infer_cmd}\\n\")\n",
    "        print(f\"Simplifying: {os.path.basename(sample_mesh)}\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        !{infer_cmd}\n",
    "        \n",
    "        # Check if output was created\n",
    "        if os.path.exists(output_mesh):\n",
    "            output_size = os.path.getsize(output_mesh) / 1024\n",
    "            input_size = os.path.getsize(sample_mesh) / 1024\n",
    "            print(f\"\\nSimplification complete!\")\n",
    "            print(f\"Input size: {input_size:.1f} KB\")\n",
    "            print(f\"Output size: {output_size:.1f} KB\")\n",
    "            print(f\"Compression ratio: {input_size/output_size:.2f}x\")\n",
    "        else:\n",
    "            print(\"\\nInference failed - no output file generated.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No sample meshes found in data/raw/ for inference test.\")\n",
    "else:\n",
    "    print(\"No checkpoint available for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-tips",
   "metadata": {},
   "source": [
    "### Training Tips for Colab T4\n",
    "\n",
    "**For resuming interrupted training:**\n",
    "1. Set `resume_checkpoint` to the path of your latest checkpoint\n",
    "2. Re-run the training cell\n",
    "\n",
    "**For Google Drive persistence:**\n",
    "1. Uncomment the Google Drive mounting code\n",
    "2. Set `checkpoint_dir` to your Drive folder\n",
    "3. Your checkpoints will survive Colab session restarts\n",
    "\n",
    "**Memory management:**\n",
    "- The default batch size (2) is optimized for T4's ~15GB memory\n",
    "- If you get OOM errors, reduce batch size in `configs/default.yaml`\n",
    "- Monitor GPU memory usage with the included monitoring\n",
    "\n",
    "**Training monitoring:**\n",
    "- Debug logs provide detailed training progress\n",
    "- Resource monitoring tracks CPU/GPU usage\n",
    "- Checkpoints are saved automatically for recovery"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
